#!/usr/bin/env python3
"""
Video to Dataset Extractor - PuzzleBot
Extrae frames de videos del PuzzleBot para crear dataset de entrenamiento
"""

import cv2
import numpy as np
import os
import json
from pathlib import Path
import shutil
from datetime import datetime
import tkinter as tk
from tkinter import filedialog, messagebox, simpledialog
from PIL import Image, ImageTk

class VideoDatasetExtractor:
    def __init__(self):
        self.output_dir = Path("extracted_dataset")
        self.classes = [
            'stop',         # 0
            'road_work',    # 1  
            'give_way',     # 2
            'turn_left',    # 3
            'go_straight',  # 4
            'turn_right'    # 5
        ]
        
        # Configuraci√≥n optimizada para videos cortos con c√°mara de amplio campo de visi√≥n
        self.frame_skip = 5   # Extraer cada 5 frames (m√°s denso para videos cortos)
        self.min_frame_diff = 20  # Diferencia m√≠nima entre frames (m√°s sensible)
        self.blur_threshold = 100   # Umbral est√°ndar (imagen ya viene corregida)
        self.brightness_range = (30, 220)  # Rango est√°ndar para buena iluminaci√≥n
        self.fisheye_correction = False  # DESACTIVADO - imagen ya viene corregida
        
        self.setup_directories()
        print("üé• Video Dataset Extractor inicializado")

    def setup_directories(self):
        """Crear estructura de directorios"""
        dirs = [
            'videos',           # Videos originales
            'extracted_frames', # Frames extra√≠dos temporalmente
            'dataset/images',   # Im√°genes finales
            'dataset/labels',   # Etiquetas YOLO
            'previews',         # Previews para verificaci√≥n
        ]
        
        for dir_path in dirs:
            (self.output_dir / dir_path).mkdir(parents=True, exist_ok=True)
        
        print(f"üìÅ Directorios creados en: {self.output_dir}")

    def extract_frames_from_video(self, video_path, class_name, output_subdir=None):
        """Extraer frames de un video espec√≠fico"""
        video_path = Path(video_path)
        if not video_path.exists():
            print(f"‚ùå Video no encontrado: {video_path}")
            return []
        
        print(f"\nüé• Procesando video: {video_path.name}")
        print(f"üéØ Clase: {class_name}")
        
        # Crear subdirectorio si se especifica
        if output_subdir:
            frames_dir = self.output_dir / 'extracted_frames' / output_subdir
        else:
            frames_dir = self.output_dir / 'extracted_frames' / class_name
        
        frames_dir.mkdir(parents=True, exist_ok=True)
        
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            print(f"‚ùå Error abriendo video: {video_path}")
            return []
        
        # Obtener informaci√≥n del video
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        duration = total_frames / fps if fps > 0 else 0
        
        print(f"üìä Video info: {total_frames} frames, {fps:.1f} FPS, {duration:.1f}s")
        
        extracted_frames = []
        frame_count = 0
        saved_count = 0
        last_saved_frame = None
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # Saltar frames seg√∫n configuraci√≥n
            if frame_count % self.frame_skip != 0:
                continue
            
            # Evaluar calidad del frame
            quality_score = self._evaluate_frame_quality(frame)
            
            if quality_score > 0.5:  # Umbral m√°s bajo para videos cortos
                # Verificar diferencia con frame anterior
                if self._is_frame_different(frame, last_saved_frame):
                    
                    # No aplicar correcci√≥n fish-eye (imagen ya viene corregida)
                    final_frame = frame
                    
                    # Guardar frame
                    timestamp = frame_count / fps if fps > 0 else frame_count
                    frame_name = f"{class_name}_{saved_count:04d}_t{timestamp:.2f}.jpg"
                    frame_path = frames_dir / frame_name
                    
                    cv2.imwrite(str(frame_path), final_frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
                    
                    extracted_frames.append({
                        'path': str(frame_path),
                        'frame_number': frame_count,
                        'timestamp': timestamp,
                        'quality_score': quality_score,
                        'class': class_name,
                        'corrected': False  # No se aplic√≥ correcci√≥n
                    })
                    
                    last_saved_frame = frame.copy()
                    saved_count += 1
                    
                    if saved_count % 10 == 0:  # Reportar cada 10 frames
                        print(f"   üì∏ Extra√≠dos: {saved_count} frames")
                        
            # Para videos cortos, mostrar progreso m√°s frecuentemente
            if frame_count % 50 == 0:
                progress = (frame_count / total_frames) * 100 if total_frames > 0 else 0
                print(f"   üé¨ Progreso: {progress:.1f}% ({frame_count}/{total_frames})")
        
        cap.release()
        print(f"‚úÖ Completado: {saved_count} frames extra√≠dos de {total_frames}")
        
        return extracted_frames

    def apply_fisheye_correction(self, frame):
        """Aplicar correcci√≥n b√°sica para lente fish-eye"""
        if not self.fisheye_correction:
            return frame
        
        try:
            h, w = frame.shape[:2]
            
            # Par√°metros b√°sicos de correcci√≥n fish-eye
            # Estos valores pueden necesitar ajuste seg√∫n tu lente espec√≠fica
            map1, map2 = cv2.fisheye.initUndistortRectifyMap(
                K=np.array([[w*0.8, 0, w/2], [0, h*0.8, h/2], [0, 0, 1]], dtype=np.float32),
                D=np.array([0.1, 0.05, 0.01, 0.0], dtype=np.float32),  # Coeficientes de distorsi√≥n
                R=np.eye(3),
                P=np.array([[w*0.7, 0, w/2], [0, h*0.7, h/2], [0, 0, 1]], dtype=np.float32),
                size=(w, h),
                m1type=cv2.CV_32FC1
            )
            
            corrected = cv2.remap(frame, map1, map2, cv2.INTER_LINEAR)
            return corrected
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error en correcci√≥n fish-eye: {e}")
            return frame

    def detect_traffic_signs_region(self, frame):
        """Detectar regiones donde probablemente est√©n las se√±ales de tr√°fico"""
        h, w = frame.shape[:2]
        
        # Para c√°mara con amplio campo de visi√≥n, las se√±ales pueden estar en varias posiciones
        # Regi√≥n m√°s amplia que cubre la parte central-superior donde t√≠picamente aparecen
        roi_top = int(h * 0.05)      # 5% desde arriba
        roi_bottom = int(h * 0.75)   # hasta 75% de la altura
        roi_left = int(w * 0.15)     # 15% desde la izquierda  
        roi_right = int(w * 0.85)    # hasta 85% del ancho
        
        # Crear m√°scara para la ROI
        mask = np.zeros((h, w), dtype=np.uint8)
        mask[roi_top:roi_bottom, roi_left:roi_right] = 255
        
        return mask, (roi_left, roi_top, roi_right, roi_bottom)
    def _evaluate_frame_quality(self, frame):
        """Evaluar calidad del frame optimizado para fish-eye y videos cortos"""
        if frame is None:
            return 0.0
        
        # Aplicar correcci√≥n fish-eye si est√° habilitada
        corrected_frame = self.apply_fisheye_correction(frame)
        
        # Convertir a escala de grises
        gray = cv2.cvtColor(corrected_frame, cv2.COLOR_BGR2GRAY)
        
        # Obtener regi√≥n de inter√©s para se√±ales
        roi_mask, roi_coords = self.detect_traffic_signs_region(corrected_frame)
        roi_left, roi_top, roi_right, roi_bottom = roi_coords
        
        # Evaluar solo en la regi√≥n de inter√©s
        roi_gray = gray[roi_top:roi_bottom, roi_left:roi_right]
        
        # 1. Detectar blur usando varianza del Laplaciano (ajustado para fish-eye)
        laplacian_var = cv2.Laplacian(roi_gray, cv2.CV_64F).var()
        blur_score = min(laplacian_var / self.blur_threshold, 1.0)
        
        # 2. Evaluar brillo en ROI
        mean_brightness = np.mean(roi_gray)
        if self.brightness_range[0] <= mean_brightness <= self.brightness_range[1]:
            brightness_score = 1.0
        else:
            if mean_brightness < self.brightness_range[0]:
                brightness_score = mean_brightness / self.brightness_range[0]
            else:
                brightness_score = (255 - mean_brightness) / (255 - self.brightness_range[1])
            brightness_score = max(0, brightness_score)
        
        # 3. Evaluar contraste en ROI
        contrast_score = roi_gray.std() / 128.0
        contrast_score = min(contrast_score, 1.0)
        
        # 4. Detectar formas circulares/rectangulares (t√≠picas de se√±ales)
        edges = cv2.Canny(roi_gray, 30, 100)  # Umbrales m√°s bajos para fish-eye
        
        # Detectar c√≠rculos (se√±ales circulares)
        circles = cv2.HoughCircles(roi_gray, cv2.HOUGH_GRADIENT, dp=1, minDist=30,
                                 param1=50, param2=30, minRadius=10, maxRadius=100)
        
        # Detectar contornos (se√±ales rectangulares/triangulares)
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # Evaluar presencia de formas geom√©tricas
        shape_score = 0.0
        if circles is not None:
            shape_score += min(len(circles[0]) * 0.3, 1.0)
        
        # Filtrar contornos por √°rea y forma
        geometric_shapes = 0
        for contour in contours:
            area = cv2.contourArea(contour)
            if 100 < area < 5000:  # √Årea razonable para se√±ales
                # Aproximar contorno
                epsilon = 0.02 * cv2.arcLength(contour, True)
                approx = cv2.approxPolyDP(contour, epsilon, True)
                
                # Si tiene 3-8 v√©rtices, podr√≠a ser una se√±al
                if 3 <= len(approx) <= 8:
                    geometric_shapes += 1
        
        shape_score += min(geometric_shapes * 0.2, 1.0)
        shape_score = min(shape_score, 1.0)
        
        # 5. Evaluar densidad de bordes en ROI
        edge_density = np.sum(edges > 0) / edges.size
        content_score = min(edge_density * 15, 1.0)  # Factor ajustado para fish-eye
        
        # Combinar scores con pesos optimizados para detecci√≥n de se√±ales
        final_score = (
            blur_score * 0.35 +        # Menos peso al blur (fish-eye puede ser menos n√≠tida)
            brightness_score * 0.25 +   # Brillo sigue siendo importante
            contrast_score * 0.15 +     # Contraste moderado
            content_score * 0.1 +       # Contenido general
            shape_score * 0.15          # NUEVO: Presencia de formas geom√©tricas
        )
        
        return final_score

    def _is_frame_different(self, frame1, frame2):
        """Verificar si dos frames son suficientemente diferentes"""
        if frame2 is None:
            return True
        
        # Redimensionar para comparaci√≥n r√°pida
        small1 = cv2.resize(frame1, (64, 64))
        small2 = cv2.resize(frame2, (64, 64))
        
        # Calcular diferencia
        diff = cv2.absdiff(small1, small2)
        mean_diff = np.mean(diff)
        
        return mean_diff > self.min_frame_diff

    def manual_frame_selection(self, video_path, class_name):
        """Selecci√≥n manual de frames usando interfaz gr√°fica"""
        print(f"\nüñ±Ô∏è Selecci√≥n manual para: {class_name}")
        
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            print(f"‚ùå Error abriendo video: {video_path}")
            return []
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        # Crear ventana de selecci√≥n
        root = tk.Tk()
        root.title(f"Selecci√≥n Manual - {class_name}")
        root.geometry("800x700")
        
        selected_frames = []
        current_frame_idx = [0]
        
        # Variables de la interfaz
        frame_label = tk.Label(root)
        frame_label.pack(pady=10)
        
        info_label = tk.Label(root, text="", font=("Arial", 10))
        info_label.pack()
        
        def update_frame_display():
            cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame_idx[0])
            ret, frame = cap.read()
            
            if ret:
                # Redimensionar para mostrar
                display_frame = cv2.resize(frame, (640, 480))
                display_frame_rgb = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)
                
                # Convertir a formato Tkinter
                pil_image = Image.fromarray(display_frame_rgb)
                tk_image = ImageTk.PhotoImage(pil_image)
                
                frame_label.configure(image=tk_image)
                frame_label.image = tk_image
                
                # Actualizar info
                timestamp = current_frame_idx[0] / fps if fps > 0 else current_frame_idx[0]
                quality = self._evaluate_frame_quality(frame)
                info_text = f"Frame: {current_frame_idx[0]}/{total_frames} | Tiempo: {timestamp:.2f}s | Calidad: {quality:.2f}"
                info_label.configure(text=info_text)
                
                return frame
            return None
        
        def save_current_frame():
            frame = update_frame_display()
            if frame is not None:
                timestamp = current_frame_idx[0] / fps if fps > 0 else current_frame_idx[0]
                frame_name = f"{class_name}_manual_{len(selected_frames):04d}_t{timestamp:.2f}.jpg"
                
                frames_dir = self.output_dir / 'extracted_frames' / class_name
                frames_dir.mkdir(parents=True, exist_ok=True)
                frame_path = frames_dir / frame_name
                
                cv2.imwrite(str(frame_path), frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
                
                selected_frames.append({
                    'path': str(frame_path),
                    'frame_number': current_frame_idx[0],
                    'timestamp': timestamp,
                    'quality_score': self._evaluate_frame_quality(frame),
                    'class': class_name
                })
                
                print(f"üíæ Guardado: {frame_name}")
                messagebox.showinfo("Guardado", f"Frame guardado: {frame_name}")
        
        def next_frame():
            if current_frame_idx[0] < total_frames - 1:
                current_frame_idx[0] += 1
                update_frame_display()
        
        def prev_frame():
            if current_frame_idx[0] > 0:
                current_frame_idx[0] -= 1
                update_frame_display()
        
        def jump_frame():
            new_frame = simpledialog.askinteger("Saltar", f"Frame (0-{total_frames-1}):")
            if new_frame is not None and 0 <= new_frame < total_frames:
                current_frame_idx[0] = new_frame
                update_frame_display()
        
        def skip_frames(n):
            current_frame_idx[0] = min(current_frame_idx[0] + n, total_frames - 1)
            update_frame_display()
        
        # Botones de control
        controls_frame = tk.Frame(root)
        controls_frame.pack(pady=10)
        
        tk.Button(controls_frame, text="‚óÄ‚óÄ -50", command=lambda: skip_frames(-50)).pack(side=tk.LEFT, padx=2)
        tk.Button(controls_frame, text="‚óÄ -10", command=lambda: skip_frames(-10)).pack(side=tk.LEFT, padx=2)
        tk.Button(controls_frame, text="‚óÄ Anterior", command=prev_frame).pack(side=tk.LEFT, padx=2)
        tk.Button(controls_frame, text="üíæ GUARDAR", command=save_current_frame, bg="lightgreen").pack(side=tk.LEFT, padx=5)
        tk.Button(controls_frame, text="Siguiente ‚ñ∂", command=next_frame).pack(side=tk.LEFT, padx=2)
        tk.Button(controls_frame, text="+10 ‚ñ∂", command=lambda: skip_frames(10)).pack(side=tk.LEFT, padx=2)
        tk.Button(controls_frame, text="+50 ‚ñ∂‚ñ∂", command=lambda: skip_frames(50)).pack(side=tk.LEFT, padx=2)
        
        tk.Button(root, text="üîç Saltar a Frame", command=jump_frame).pack(pady=5)
        tk.Button(root, text="‚úÖ Terminar", command=root.quit, bg="lightcoral").pack(pady=10)
        
        # Mostrar primer frame
        update_frame_display()
        
        # Ejecutar interfaz
        root.mainloop()
        root.destroy()
        
        cap.release()
        print(f"‚úÖ Selecci√≥n manual completada: {len(selected_frames)} frames")
        
        return selected_frames

    def create_smart_bbox_annotations(self, frames_data):
        """Crear anotaciones inteligentes basadas en detecci√≥n de formas"""
        print(f"\nü§ñ Creando anotaciones inteligentes...")
        
        annotated_data = []
        
        for frame_info in frames_data:
            frame_path = Path(frame_info['path'])
            class_name = frame_info['class']
            class_id = self.classes.index(class_name)
            
            # Cargar imagen
            img = cv2.imread(str(frame_path))
            if img is None:
                continue
            
            h, w = img.shape[:2]
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            
            # Obtener regi√≥n de inter√©s
            roi_mask, roi_coords = self.detect_traffic_signs_region(img)
            roi_left, roi_top, roi_right, roi_bottom = roi_coords
            roi_gray = gray[roi_top:roi_bottom, roi_left:roi_right]
            
            # Intentar detectar formas autom√°ticamente
            best_bbox = None
            
            # 1. Para se√±al STOP (octagonal) - detectar como c√≠rculo aproximado
            if class_name == 'stop':
                circles = cv2.HoughCircles(roi_gray, cv2.HOUGH_GRADIENT, dp=1, minDist=60,
                                         param1=50, param2=30, minRadius=25, maxRadius=150)
                
                if circles is not None and len(circles[0]) > 0:
                    # Tomar el c√≠rculo m√°s grande y bien definido
                    circle = max(circles[0], key=lambda c: c[2])
                    cx, cy, radius = circle
                    
                    # Convertir coordenadas relativas a imagen completa
                    abs_cx = (cx + roi_left) / w
                    abs_cy = (cy + roi_top) / h
                    bbox_w = (radius * 2.3) / w  # Un poco m√°s grande para cubrir el oct√°gono
                    bbox_h = (radius * 2.3) / h
                    
                    # Asegurar que est√© dentro de l√≠mites
                    bbox_w = min(bbox_w, 0.7)
                    bbox_h = min(bbox_h, 0.7)
                    
                    best_bbox = [abs_cx, abs_cy, bbox_w, bbox_h]
            
            # 2. Para se√±ales circulares con flechas
            elif class_name in ['turn_left', 'go_straight', 'turn_right']:
                circles = cv2.HoughCircles(roi_gray, cv2.HOUGH_GRADIENT, dp=1, minDist=50,
                                         param1=50, param2=30, minRadius=20, maxRadius=130)
                
                if circles is not None and len(circles[0]) > 0:
                    circle = max(circles[0], key=lambda c: c[2])
                    cx, cy, radius = circle
                    
                    abs_cx = (cx + roi_left) / w
                    abs_cy = (cy + roi_top) / h
                    bbox_w = (radius * 2.2) / w
                    bbox_h = (radius * 2.2) / h
                    
                    bbox_w = min(bbox_w, 0.6)
                    bbox_h = min(bbox_h, 0.6)
                    
                    best_bbox = [abs_cx, abs_cy, bbox_w, bbox_h]
            
            # 3. Para se√±ales triangulares
            elif class_name in ['road_work', 'give_way']:
                edges = cv2.Canny(roi_gray, 50, 150)
                contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                
                triangular_contours = []
                for contour in contours:
                    area = cv2.contourArea(contour)
                    if 800 < area < 12000:  # √Årea razonable para tri√°ngulos
                        epsilon = 0.02 * cv2.arcLength(contour, True)
                        approx = cv2.approxPolyDP(contour, epsilon, True)
                        
                        if len(approx) >= 3 and len(approx) <= 4:  # Tri√°ngulo o aproximaci√≥n
                            triangular_contours.append((contour, area))
                
                if triangular_contours:
                    best_contour = max(triangular_contours, key=lambda x: x[1])[0]
                    x, y, w_cont, h_cont = cv2.boundingRect(best_contour)
                    
                    abs_cx = ((x + w_cont/2) + roi_left) / w
                    abs_cy = ((y + h_cont/2) + roi_top) / h
                    bbox_w = (w_cont * 1.4) / w
                    bbox_h = (h_cont * 1.4) / h
                    
                    bbox_w = min(bbox_w, 0.7)
                    bbox_h = min(bbox_h, 0.7)
                    
                    best_bbox = [abs_cx, abs_cy, bbox_w, bbox_h]
            
            # 4. Si no se detect√≥ nada, usar bbox por defecto optimizada por clase
            if best_bbox is None:
                if class_name == 'stop':
                    # STOP suele estar prominente y centrado
                    best_bbox = [0.5, 0.4, 0.5, 0.5]
                elif class_name in ['turn_left', 'go_straight', 'turn_right']:
                    # Se√±ales de direcci√≥n suelen estar en posici√≥n media
                    best_bbox = [0.5, 0.35, 0.4, 0.4]
                elif class_name in ['road_work', 'give_way']:
                    # Se√±ales triangulares pueden estar un poco m√°s arriba
                    best_bbox = [0.5, 0.3, 0.4, 0.45]
                else:
                    # Default gen√©rico
                    best_bbox = [0.5, 0.4, 0.45, 0.45]
            
            # Validar y ajustar bbox si es necesario
            center_x, center_y, bbox_width, bbox_height = best_bbox
            
            # Asegurar que est√© completamente dentro de la imagen
            half_w = bbox_width / 2
            half_h = bbox_height / 2
            
            center_x = max(half_w, min(1 - half_w, center_x))
            center_y = max(half_h, min(1 - half_h, center_y))
            
            if (0 < center_x < 1 and 0 < center_y < 1 and 
                0 < bbox_width < 1 and 0 < bbox_height < 1):
                
                # Crear archivo de etiqueta YOLO
                label_name = frame_path.stem + '.txt'
                label_path = frame_path.parent / label_name
                
                with open(label_path, 'w') as f:
                    f.write(f"{class_id} {center_x:.6f} {center_y:.6f} {bbox_width:.6f} {bbox_height:.6f}\n")
                
                annotated_data.append({
                    'image_path': str(frame_path),
                    'label_path': str(label_path),
                    'class': class_name,
                    'bbox': [center_x, center_y, bbox_width, bbox_height],
                    'detection_method': 'smart_auto'
                })
                
                print(f"   üéØ {class_name}: bbox creada ({center_x:.2f}, {center_y:.2f}) {bbox_width:.2f}x{bbox_height:.2f}")
            else:
                print(f"   ‚ö†Ô∏è {class_name}: bbox inv√°lida")
        
        print(f"‚úÖ Creadas {len(annotated_data)} anotaciones inteligentes")
        return annotated_data
        """Crear anotaciones de bounding boxes para frames extra√≠dos"""
        print(f"\nüì¶ Creando anotaciones de bounding boxes...")
        
        annotated_data = []
        
        for frame_info in frames_data:
            frame_path = Path(frame_info['path'])
            class_name = frame_info['class']
            class_id = self.classes.index(class_name)
            
            # Cargar imagen para obtener dimensiones
            img = cv2.imread(str(frame_path))
            if img is None:
                continue
            
            h, w = img.shape[:2]
            
            # Por defecto, crear bbox que cubra el centro de la imagen
            # Esto asume que la se√±al est√° principalmente en el centro
            center_x = 0.5
            center_y = 0.5
            bbox_width = 0.6   # 60% del ancho
            bbox_height = 0.6  # 60% del alto
            
            # Crear archivo de etiqueta YOLO
            label_name = frame_path.stem + '.txt'
            label_path = frame_path.parent / label_name
            
            with open(label_path, 'w') as f:
                f.write(f"{class_id} {center_x:.6f} {center_y:.6f} {bbox_width:.6f} {bbox_height:.6f}\n")
            
            annotated_data.append({
                'image_path': str(frame_path),
                'label_path': str(label_path),
                'class': class_name,
                'bbox': [center_x, center_y, bbox_width, bbox_height]
            })
        
        print(f"‚úÖ Creadas {len(annotated_data)} anotaciones autom√°ticas")
        return annotated_data

    def create_manual_bbox_annotations(self, frames_data):
        """Crear anotaciones de bounding boxes manualmente usando interfaz gr√°fica"""
        print(f"\nüñ±Ô∏è Anotaci√≥n manual de bounding boxes...")
        
        annotated_data = []
        
        for i, frame_info in enumerate(frames_data):
            frame_path = Path(frame_info['path'])
            class_name = frame_info['class']
            class_id = self.classes.index(class_name)
            
            print(f"\nüì¶ Anotando {i+1}/{len(frames_data)}: {frame_path.name}")
            
            # Cargar imagen
            img = cv2.imread(str(frame_path))
            if img is None:
                continue
            
            original_h, original_w = img.shape[:2]
            
            # Redimensionar para mostrar (manteniendo aspecto)
            display_scale = min(800 / original_w, 600 / original_h)
            display_w = int(original_w * display_scale)
            display_h = int(original_h * display_scale)
            display_img = cv2.resize(img, (display_w, display_h))
            
            # Variables para el bbox
            bbox_coords = {'start': None, 'end': None, 'drawing': False}
            current_img = display_img.copy()
            
            def mouse_callback(event, x, y, flags, param):
                nonlocal current_img
                
                if event == cv2.EVENT_LBUTTONDOWN:
                    bbox_coords['start'] = (x, y)
                    bbox_coords['drawing'] = True
                    
                elif event == cv2.EVENT_MOUSEMOVE and bbox_coords['drawing']:
                    current_img = display_img.copy()
                    cv2.rectangle(current_img, bbox_coords['start'], (x, y), (0, 255, 0), 2)
                    cv2.imshow('Anotar BBox', current_img)
                    
                elif event == cv2.EVENT_LBUTTONUP:
                    bbox_coords['end'] = (x, y)
                    bbox_coords['drawing'] = False
                    cv2.rectangle(current_img, bbox_coords['start'], bbox_coords['end'], (0, 255, 0), 2)
                    cv2.imshow('Anotar BBox', current_img)
            
            # Crear ventana y configurar callback
            cv2.namedWindow('Anotar BBox', cv2.WINDOW_NORMAL)
            cv2.resizeWindow('Anotar BBox', display_w, display_h)
            cv2.setMouseCallback('Anotar BBox', mouse_callback)
            
            # Mostrar instrucciones
            instructions = current_img.copy()
            cv2.putText(instructions, f"Clase: {class_name}", (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
            cv2.putText(instructions, "Arrastra para crear bbox", (10, 70), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            cv2.putText(instructions, "ENTER: Guardar | ESC: Saltar | Q: Terminar", (10, 100), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            cv2.imshow('Anotar BBox', instructions)
            
            # Esperar interacci√≥n del usuario
            while True:
                key = cv2.waitKey(1) & 0xFF
                
                if key == 13:  # ENTER - Guardar
                    if bbox_coords['start'] and bbox_coords['end']:
                        # Convertir coordenadas a formato YOLO
                        x1 = min(bbox_coords['start'][0], bbox_coords['end'][0]) / display_scale
                        y1 = min(bbox_coords['start'][1], bbox_coords['end'][1]) / display_scale
                        x2 = max(bbox_coords['start'][0], bbox_coords['end'][0]) / display_scale
                        y2 = max(bbox_coords['start'][1], bbox_coords['end'][1]) / display_scale
                        
                        # Convertir a formato YOLO (normalizado)
                        center_x = ((x1 + x2) / 2) / original_w
                        center_y = ((y1 + y2) / 2) / original_h
                        bbox_width = (x2 - x1) / original_w
                        bbox_height = (y2 - y1) / original_h
                        
                        # Validar bbox
                        if (0 <= center_x <= 1 and 0 <= center_y <= 1 and 
                            0 < bbox_width <= 1 and 0 < bbox_height <= 1):
                            
                            # Crear archivo de etiqueta
                            label_name = frame_path.stem + '.txt'
                            label_path = frame_path.parent / label_name
                            
                            with open(label_path, 'w') as f:
                                f.write(f"{class_id} {center_x:.6f} {center_y:.6f} {bbox_width:.6f} {bbox_height:.6f}\n")
                            
                            annotated_data.append({
                                'image_path': str(frame_path),
                                'label_path': str(label_path),
                                'class': class_name,
                                'bbox': [center_x, center_y, bbox_width, bbox_height]
                            })
                            
                            print(f"   ‚úÖ Guardado: {frame_path.name}")
                            break
                        else:
                            print(f"   ‚ö†Ô∏è BBox inv√°lida, intenta de nuevo")
                            bbox_coords = {'start': None, 'end': None, 'drawing': False}
                            current_img = display_img.copy()
                            cv2.imshow('Anotar BBox', current_img)
                    else:
                        print(f"   ‚ö†Ô∏è Dibuja una bounding box primero")
                
                elif key == 27:  # ESC - Saltar
                    print(f"   ‚è≠Ô∏è Saltado: {frame_path.name}")
                    break
                
                elif key == ord('q'):  # Q - Terminar
                    cv2.destroyAllWindows()
                    return annotated_data
            
            cv2.destroyAllWindows()
        
        print(f"‚úÖ Anotaci√≥n manual completada: {len(annotated_data)} im√°genes")
        return annotated_data

    def organize_final_dataset(self, annotated_data):
        """Organizar dataset final en estructura para el script de entrenamiento"""
        print(f"\nüìã Organizando dataset final...")
        
        # Crear estructura final
        final_images_dir = self.output_dir / 'dataset' / 'images'
        final_labels_dir = self.output_dir / 'dataset' / 'labels'
        
        final_images_dir.mkdir(parents=True, exist_ok=True)
        final_labels_dir.mkdir(parents=True, exist_ok=True)
        
        organized_count = 0
        
        for data in annotated_data:
            src_img_path = Path(data['image_path'])
            src_label_path = Path(data['label_path'])
            
            if src_img_path.exists() and src_label_path.exists():
                # Crear nombres √∫nicos
                final_name = f"{data['class']}_{organized_count:04d}.jpg"
                
                # Copiar imagen
                dst_img_path = final_images_dir / final_name
                shutil.copy2(src_img_path, dst_img_path)
                
                # Copiar etiqueta
                dst_label_path = final_labels_dir / (final_name.replace('.jpg', '.txt'))
                shutil.copy2(src_label_path, dst_label_path)
                
                organized_count += 1
        
        print(f"‚úÖ Dataset organizado: {organized_count} pares imagen-etiqueta")
        
        # Crear resumen
        summary = {
            'total_images': organized_count,
            'classes': self.classes,
            'extraction_date': datetime.now().isoformat(),
            'dataset_structure': 'images/ + labels/',
        }
        
        summary_path = self.output_dir / 'dataset_summary.json'
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"üìÑ Resumen guardado: {summary_path}")
        
        return organized_count

    def process_videos_batch(self, videos_info, extraction_mode='auto'):
        """Procesar m√∫ltiples videos en lote"""
        print(f"\nüé¨ Procesando {len(videos_info)} videos en modo: {extraction_mode}")
        
        all_frames = []
        
        for video_info in videos_info:
            video_path = video_info['path']
            class_name = video_info['class']
            
            if extraction_mode == 'auto':
                frames = self.extract_frames_from_video(video_path, class_name)
            elif extraction_mode == 'manual':
                frames = self.manual_frame_selection(video_path, class_name)
            else:
                print(f"‚ùå Modo de extracci√≥n desconocido: {extraction_mode}")
                continue
            
            all_frames.extend(frames)
        
        print(f"\nüìä Total frames extra√≠dos: {len(all_frames)}")
        
        return all_frames

def main():
    """Pipeline principal para extracci√≥n de dataset desde videos"""
    print("=" * 70)
    print("üé• VIDEO TO DATASET EXTRACTOR - PUZZLEBOT")
    print("=" * 70)
    
    extractor = VideoDatasetExtractor()
    
    # üéØ RUTAS PREDEFINIDAS DE LOS VIDEOS
    base_path = "/home/strpicket/Videos/Screencasts/Pista_prueba/IOS"
    predefined_videos = {
        'stop': f"{base_path}/Stop_ios.mp4",
        'road_work': f"{base_path}/Road_work_ios.mp4", 
        'give_way': f"{base_path}/Give_way_ios.mp4",
        'turn_left': f"{base_path}/Turn_left_ios.mp4",
        'go_straight': f"{base_path}/Go_straight_ios.mp4",
        'turn_right': f"{base_path}/Turn_right_ios.mp4"
    }
    
    print(f"\nüìÅ Videos encontrados en: {base_path}")
    
    videos_info = []
    
    # Verificar cada video y configurar autom√°ticamente
    for class_name in extractor.classes:
        video_path = predefined_videos.get(class_name)
        
        if video_path and Path(video_path).exists():
            videos_info.append({
                'path': video_path,
                'class': class_name
            })
            print(f"   ‚úÖ {class_name}: {Path(video_path).name}")
        else:
            print(f"   ‚ùå {class_name}: No encontrado - {video_path}")
    
    if not videos_info:
        print("‚ùå No se encontraron videos v√°lidos")
        return
    
    print(f"\nüìä Videos configurados autom√°ticamente: {len(videos_info)}")
    for video in videos_info:
        print(f"   üé¨ {video['class']}: {Path(video['path']).name}")
    
    # Confirmar antes de proceder
    confirm = input(f"\nüöÄ ¬øProceder con estos {len(videos_info)} videos? (y/n) [y]: ").strip().lower()
    if confirm == 'n':
        print("‚ùå Cancelado por el usuario")
        return
    
    # Seleccionar modo de extracci√≥n
    print(f"\nüîß Modos de extracci√≥n disponibles:")
    print("1. auto   - Extracci√≥n autom√°tica (recomendado)")
    print("2. manual - Selecci√≥n manual de frames")
    
    mode = input("Selecciona modo (auto/manual) [auto]: ").strip().lower() or 'auto'
    
    if mode not in ['auto', 'manual']:
        print("‚ùå Modo inv√°lido, usando 'auto'")
        mode = 'auto'
    
    # Procesar videos
    all_frames = extractor.process_videos_batch(videos_info, mode)
    
    if not all_frames:
        print("‚ùå No se extrajeron frames")
        return
    
    # Seleccionar modo de anotaci√≥n
    print(f"\nüîß Modos de anotaci√≥n disponibles:")
    print("1. smart  - Detecci√≥n autom√°tica de formas (RECOMENDADO para fish-eye)")
    print("2. auto   - Bounding boxes autom√°ticas simples")
    print("3. manual - Dibujar bounding boxes manualmente")
    
    annotation_mode = input("Selecciona modo de anotaci√≥n (smart/auto/manual) [smart]: ").strip().lower() or 'smart'
    
    # Crear anotaciones
    if annotation_mode == 'manual':
        annotated_data = extractor.create_manual_bbox_annotations(all_frames)
    elif annotation_mode == 'smart':
        annotated_data = extractor.create_smart_bbox_annotations(all_frames)
    else:
        annotated_data = extractor.create_bounding_box_annotations(all_frames)
    
    if not annotated_data:
        print("‚ùå No se crearon anotaciones")
        return
    
    # Organizar dataset final
    final_count = extractor.organize_final_dataset(annotated_data)
    
    # Resumen final
    print("\n" + "="*70)
    print("üéâ EXTRACCI√ìN COMPLETADA")
    print("="*70)
    print(f"üìä Total im√°genes: {final_count}")
    print(f"üìÅ Dataset en: {extractor.output_dir}/dataset/")
    print(f"   üì∑ Im√°genes: {extractor.output_dir}/dataset/images/")
    print(f"   üìÑ Etiquetas: {extractor.output_dir}/dataset/labels/")
    
    print(f"\nüìù PR√ìXIMOS PASOS:")
    print(f"1. Copiar dataset al script de entrenamiento:")
    print(f"   cp -r {extractor.output_dir}/dataset/* puzzlebot_real_training/input_dataset/")
    print(f"2. Ejecutar script de entrenamiento con dataset real")
    print(f"3. ¬°Entrenar el modelo!")

if __name__ == "__main__":
    main()